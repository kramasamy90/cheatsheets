\section{Vector Spaces}

\subsection{Vector spaces and subspaces}

\textbf{Vector space}
Set of vectors with vector addition and scalar multiplication, satisfying the following properties.\\
\begin{itemize}
	\item Associativity of vector addition.
	\item Commutativity of vector addition.
	\item Identity element of vector addition.
	\item Inverse elements of vector addition.
	\item Compatibility of scalar multiplication with field multiplication. i.e., $a(b\mathbf{v}) = ab(\mathbf{v})$
	\item Identity element of scalar multiplication.
	\item Distributivity of scalar multiplication w.r.t. vector addition. i.e., $a(\mathbf{u} + \mathbf{v}) = a\mathbf{u} + a\mathbf{v}$
	\item Distributivity of scalar addition w.r.t. vector multiplication. i.e. $(a + b)\mathbf{v} = a\mathbf{v} + b\mathbf{v}$
\end{itemize}

If $S$ and $P$ are two vector spaces, $S \cup P$ need not be a vector space, but $S \cap P$ is a vector space.


\textbf{Subspace}
$Q \subseteq P$ is a subspace of $P$, if it satisfies the following property in addition to all the properties of a vector space.
\begin{itemize}
	\item $\textbf{a} \in Q$ and $\textbf{b} \in Q$ $\implies$ $\textbf{a} + \textbf{b} \in Q$.	
	\item $\textbf{a} \in Q$ $\implies $ $c\textbf{a} \in Q$ for all scalar c.
\end{itemize}


\subsection{Solving $Ax = b$[Revisited]}

Augmented matrix $[A|b]$ $\rightarrow$ Row operation leading to echelon form.\\
\textbf{Pivot elements} First non-zero element in a row with a column of zeroes below it.\\
\textbf{Rank} = number of pivot elements.\\
\textbf{Free columns} = Columns without pivot elements.\\
\textbf{Null space:} of a matrix $A$, is the set of all $x$ that satisfies $Ax = 0$.\\
\textbf{General solution:} $x = x_p + x_n$, where $x_n$ is a vector from the nullspace of A.\\
The existence and number of solutions depends on the rank of $A$ and that of the augmented matrix $[A|b]$.\\


\subsection{Linear independence, basis and dimension}

\textbf{Linear independence:} A set of vectors $v_1, v_2, \cdots, v_k$ are said to be linearly independent iff\\
$c_1v_1 + c_2v_2 + \cdots + c_kv_k = 0$ $\Rightarrow$ $c_1 = c_2 = \cdots = c_k = 0$.

\textbf{Linear Combination:} $LC(v_i) = \{u \mid u = \sum_j c_jv_j\}$


\textbf{Span} of a set of vectors $\{v_1, v_2, \cdots, v_k\} \subset V$ 
is the smallest subspace $U \subseteq V$ containng these vectors.

\begin{mdframed}[backgroundcolor=SlateGray2!40,linecolor=Firebrick4]
\textbf{Theorem:} $LC(\{v_i\}) = Span(\{v_i\})$.
\end{mdframed}

\textbf{Basis:} Basis of a vector space is a set of vectors with the following 
                two properties.\\
\begin{itemize}
	\item The vectors in the set are linearly independent.
	\item They span the vector space.
\end{itemize}

\begin{mdframed}[backgroundcolor=SlateGray2!40,linecolor=Firebrick4]
\textbf{Theorems}
\begin{itemize}
\item If $A \subseteq V$ is linearly independent and $B \subseteq V$ is a basis
        of $V$, then $|A| \leq |B|$.
\item If $B^i \subseteq V$ and $B^j \subseteq V$ are each a basis of $V$ then 
        $|B^i| = |B^j|$.
\end{itemize}
\end{mdframed}

\textbf{Dimension of a vector space:} Size of the basis of the vector space.

\subsection{Four fundamental subspaces}
For a matrix $A$ of dimensions $m \times n$ and rank $r$.
\begin{itemize}
	\item Column space of A, $C(A)$: Smallest subspace containing the columns of A.\\
	$C(A) \subseteq \mathbb{R}^m$.\\
	Dimension of $C(A)$ is $r$.
	\item Row space of A, $R(A)$: Smallest subspace containing the rows of A.\\
	$R(A) \subseteq \mathbb{R}^n.$\\ 
	Dimension of $R(A)$ is $r$.
	\item Null space of A, $N(A)$: Set of all $x$ such that $Ax = 0$.\\
	$N(A) \subseteq \mathbb{R}^n$.\\
	Dimension of $N(A)$ is $n-r$.
	\item Null space of $A^T$, $N(A^T)$: Set of all $x$ such that $A^Tx = 0$.\\
	$N(A^T) \subseteq \mathbb{R}^m$.\\
	Dimension of $N(A^T)$ is $m-r$.
\end{itemize}

\begin{mdframed}[backgroundcolor=SlateGray2!40,linecolor=Firebrick4]
\textbf{Relationship between the subspaces:}
\begin{itemize}
	\item $C(A) \perp N(A^T)$ and $C(A) \cup N(A^T) = \mathbb{R}^m$.
	\item $R(A) \perp N(A)$ and $R(A) \cup N(A) = \mathbb{R}^n$.
    \item $C(A)$ and $R(A)$ are isomorphic.
    \item In otherwords, $A$ is invertible within the domain / co-domain of 
            $C(A)$ and $R(A)$.
\end{itemize}
\textit{Proof outline:} If $x \notin R(A)$, find $x_{\perp} \perp R(A)$ and
show that $Ax_{\perp} = 0$.


\end{mdframed}

\subsection{Linear Transformation}

\textbf{Linear Transformation:} A transformation $T$ that converts a vector $u$ to $v (= T(x))$, is a linear transformation if it satisfies the following properties.
\begin{itemize}
	\item $T(0) = 0$.
	\item $T(u + v) = T(u) + T(v)$
	\item $T(au) = aT(u)$
\end{itemize}


\textbf{Important notes:}
\begin{itemize}
\item Every matrix multiplication to a vector is a linear transformation.
\item Every linear transformation can be represented by a matrix multiplication.
\item NOTE: The transformation matrix need not be a square matrix.
\item If $Ax$ / $T(x)$ is known for all the basis of the vector space, then it is known for all the vectors in vector space.
\end{itemize}

\textbf{Examples of linear transformation}:\\
Scaling, Rotation, Reflection, Projection, Differentiation and Integration.\\


\subsection{Change of Basis and Similarity Tranform}

$x \rightarrow $ In standard basis.\\
$x_B \rightarrow$ w.r.t. the basis $B$.\\
$x_{B^1} \rightarrow$ w.r.t. the basis $B^1$.\\
$x_{B^2} \rightarrow$ w.r.t. the basis $B^2$.\\

$x_{B^1} = {B^1}^{-1} x$, or, $x = B^1x_{B^1}$\\
$x_{B^2} = {B^2}^{-1}B^1x_{B^1}$\\


\textbf{Similarity Transform:}\\

Give a transformation matrix w.r.t one basis get the equivalent transformation
another basis. \\
\textit{Eg: Standard to B:}\\
Say, $y = Ax$, we want $A'$ such that $y_{B} = A' x_B$.
$\implies By_b = ABx_b, \forall x \in V$.
$\implies y_b = B^{-1}ABx_b \implies A' = B^{-1}AB$.